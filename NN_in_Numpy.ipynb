{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3bbf6c4-9933-4920-a233-cb0f688921db",
   "metadata": {},
   "source": [
    "## Neuronales Netz Basisfunktionen in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6e397e-2378-42b5-abd5-97011a4a899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "sns.set_style(\"whitegrid\")\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225dfa3c-06e8-4dac-970d-e2d47caab266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architektur des neuronalen Netzwerks\n",
    "# wichtig ist, dass die aufeinander folgenden Layer die gleiche Größe haben\n",
    "# KANN MAN VERÄNDERN\n",
    "NN_ARCHITECTURE = [\n",
    "    {\"input_dim\": 2, \"output_dim\": 25, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 25, \"output_dim\": 50, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 50, \"output_dim\": 50, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 50, \"output_dim\": 25, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 25, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72782bf4-4b09-4b42-b98d-aafe0b8fcc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Das Netz muss am Anfang initialisiert werden \n",
    "# und das am besten nicht mit 0, weil dann kommt\n",
    "# immer 0 raus\n",
    "def init_layers(nn_architecture, seed = 99):\n",
    "    # Wahl eines Random Seed, zur Reproduzierbarkeit \n",
    "    np.random.seed(seed)\n",
    "    # Anzahl der Layer\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    # Speicher der Gewichte\n",
    "    params_values = {}\n",
    "    \n",
    "    # Schleife über alle Layer\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        # Wir starten bei 1 (ausnahmweise)\n",
    "        layer_idx = idx + 1\n",
    "        \n",
    "        # Input und output Dimensionen\n",
    "        layer_input_size = layer[\"input_dim\"]\n",
    "        layer_output_size = layer[\"output_dim\"]\n",
    "        \n",
    "        # Initialisieren der Gewichte W und der Vektoren b\n",
    "        # für jeden Layer mit normalverteilen Werten\n",
    "        params_values['W' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, layer_input_size) * 0.1\n",
    "        params_values['b' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, 1) * 0.1\n",
    "        \n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15280b5-b44f-45bc-9bdc-f8a5640903cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aktivierungsfunktionen und deren Ableitung\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0;\n",
    "    return dZ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9289c2de-98c1-4ee6-b96f-0f9c3e87eaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vorwärtsdurchlauf eines einzelnen Layers\n",
    "def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n",
    "    # Berechnung des Inputs mit den Gewichten und dem Bias\n",
    "    Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "    \n",
    "    # Auswahl der Aktivierungsfunktion\n",
    "    if activation == \"relu\":\n",
    "        activation_func = relu\n",
    "    elif activation == \"sigmoid\":\n",
    "        activation_func = sigmoid\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "        \n",
    "    # Rückgabe des neuen Inputs für das nächste Layer\n",
    "    return activation_func(Z_curr), Z_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aecb59-3b47-41e0-b773-49c136f90fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volle Berechnung des Vorwärtsdurchlauf durch das gesamte Netz\n",
    "def full_forward_propagation(X, params_values, nn_architecture):\n",
    "    # temporärer Speicher für den Rückwärtslauf\n",
    "    memory = {}\n",
    "    # Erster Input\n",
    "    A_curr = X\n",
    "    \n",
    "    # Schleife über alle Layer\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        # wir starten wieder bei 1 \n",
    "        layer_idx = idx + 1\n",
    "        # Setze den Output der letzten Iteration auf den aktuellen Wert\n",
    "        A_prev = A_curr\n",
    "        \n",
    "        # Auswahl der Aktivierungsfunktion\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        # Auswahl der entsprechenden Gewichte\n",
    "        W_curr = params_values[\"W\" + str(layer_idx)]\n",
    "        # Auswahl der entsprechenden Biases\n",
    "        b_curr = params_values[\"b\" + str(layer_idx)]\n",
    "        # Vorwärtsdurchlauf des aktuellen Layers\n",
    "        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)\n",
    "        \n",
    "        # Speichern alles wichtige für später\n",
    "        memory[\"A\" + str(idx)] = A_prev\n",
    "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
    "       \n",
    "    # Rückgabe des Outputs und der gespeicherten Werte für später\n",
    "    return A_curr, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72840fb8-f962-4bfc-a573-b4036d727b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechung der Kostenfunktion, also wie gut ist denn unser Model \n",
    "def get_cost_value(Y_hat, Y):\n",
    "    # Anzahl der Beispiele\n",
    "    m = Y_hat.shape[1]\n",
    "    # Berechnung der Kosten anhand der logistischen Regression\n",
    "    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815d70ba-247c-4eb1-8c25-628941d10da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilfsfunktion um die vorhergesagten Wahrscheinlichkeiten in die Klassen umzuwandeln\n",
    "def convert_prob_into_class(probs):\n",
    "    probs_ = np.copy(probs)\n",
    "    probs_[probs_ > 0.5] = 1\n",
    "    probs_[probs_ <= 0.5] = 0\n",
    "    return probs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b44d667-088c-4b29-a79a-ce6b8aa22add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung der Genauigkeit unseres Models\n",
    "def get_accuracy_value(Y_hat, Y):\n",
    "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
    "    return (Y_hat_ == Y).all(axis=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc58ae5-e580-4610-b79a-10fafc451f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rückwärtsdurchlauf durch ein einzelnes Layer, um die Parameter zu verbessern\n",
    "def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
    "    # Anzahl der Beispiele\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    # Auswahl der Aktivierungsfunktion\n",
    "    if activation == \"relu\":\n",
    "        backward_activation_func = relu_backward\n",
    "    elif activation == \"sigmoid\":\n",
    "        backward_activation_func = sigmoid_backward\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "    \n",
    "    # Berechnung der Ableiung der Aktivierungsfuktion\n",
    "    dZ_curr = backward_activation_func(dA_curr, Z_curr)\n",
    "    \n",
    "    # Ableitung der Gewichtsmatrix W\n",
    "    dW_curr = np.dot(dZ_curr, A_prev.T) / m\n",
    "    # Ableitung des Biasvektors b\n",
    "    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
    "    # Ableitung der Inputmatrix A_prev\n",
    "    dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "\n",
    "    return dA_prev, dW_curr, db_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2de06c-0fbb-4c9c-b30d-00650a865014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rückwärtsdurchlauf durch das gesamte Netz\n",
    "def full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture):\n",
    "    grads_values = {}\n",
    "    \n",
    "    # Anzahl der Beispiele\n",
    "    m = Y.shape[1]\n",
    "    # Sicherstellen, dass alles die gleiche Dimension hat\n",
    "    Y = Y.reshape(Y_hat.shape)\n",
    "    \n",
    "    # Erster Schritt des Gradientenabstiegs\n",
    "    dA_prev = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));\n",
    "    \n",
    "    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
    "        # Wir starten immernoch bei 1\n",
    "        layer_idx_curr = layer_idx_prev + 1\n",
    "        # Auswahl der aktuellen Aktivierungsfunktion\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "\n",
    "        # Initialisung der entsprechenden Werte\n",
    "        dA_curr = dA_prev\n",
    "        \n",
    "        A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
    "        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
    "        \n",
    "        W_curr = params_values[\"W\" + str(layer_idx_curr)]\n",
    "        b_curr = params_values[\"b\" + str(layer_idx_curr)]\n",
    "\n",
    "        # Rückwärtsdurchlauf durch das aktuelle Layer\n",
    "        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n",
    "            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)\n",
    "        \n",
    "        # Speichern der entsprchenden Gradienten \n",
    "        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
    "        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n",
    "    \n",
    "    return grads_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825d7291-1e4a-433c-9107-63a016e4acfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion um die Gewichte der Layer zu verändern\n",
    "def update(params_values, grads_values, nn_architecture, learning_rate):\n",
    "\n",
    "    # Schleiche über alle Layer\n",
    "    for layer_idx, layer in enumerate(nn_architecture, 1):\n",
    "        # Update über die Gewichte\n",
    "        params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
    "        params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c79319f-a989-4b3e-b794-ff950cf8d7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion die alles was wir vorher gebaut haben zusammenführt\n",
    "# SEED IN init_layers VERÄNDERN\n",
    "def train(X, Y, X_test, Y_test,nn_architecture, epochs, learning_rate, verbose=False):\n",
    "    # Initialisierung des Netzes\n",
    "    params_values = init_layers(nn_architecture, 2)\n",
    "    # Initialisierung, um das Training später anzuschauen \n",
    "    cost_history = []\n",
    "    accuracy_history = []\n",
    "    cost_test_history = []\n",
    "    accuracy_test_history = [] \n",
    "    \n",
    "    # Schleife über eine vorher festgelegte Anzahl an Durchlöufen (epochs)\n",
    "    for i in range(epochs):\n",
    "        # kompletter Vorwärtsschritt\n",
    "        Y_hat, cashe = full_forward_propagation(X, params_values, nn_architecture)\n",
    "        \n",
    "        # Berechnung von Kosten und Genauigkeit und speichern\n",
    "        cost = get_cost_value(Y_hat, Y)\n",
    "        cost_history.append(cost)\n",
    "        accuracy = get_accuracy_value(Y_hat, Y)\n",
    "        accuracy_history.append(accuracy)\n",
    "\n",
    "        Y_hat_test, _ = full_forward_propagation(X_test, params_values, nn_architecture)\n",
    "        cost_test = get_cost_value(Y_hat_test, Y_test)\n",
    "        cost_test_history.append(cost_test)\n",
    "        accuracy_test = get_accuracy_value(Y_hat_test, Y_test)\n",
    "        accuracy_test_history.append(accuracy_test)\n",
    "        \n",
    "        # kompletter Rückwärtsschritt um den Gradienten (Ableitung) zu berechnen\n",
    "        grads_values = full_backward_propagation(Y_hat, Y, cashe, params_values, nn_architecture)\n",
    "        # Update der Parameter im Netz\n",
    "        params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
    "\n",
    "        # Ausdrucken der aktuellen Werte des Trainings\n",
    "        if(i % 50 == 0):\n",
    "            if(verbose):\n",
    "                print(\"Iteration: {:05} - cost: {:.5f} - accuracy: {:.5f}\".format(i, cost, accuracy))\n",
    "            \n",
    "    return params_values, [cost_history, accuracy_history, cost_test_history, accuracy_test_history]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c82acf-0959-478e-90ee-aa70abd7a995",
   "metadata": {},
   "source": [
    "## Visualisierung der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a197ce4-3df4-4a56-bb8c-1036118c76de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Größe des Datensatzes\n",
    "# N_SAMPLES VERÄNDERN\n",
    "N_SAMPLES = 1000\n",
    "# Größe des Testsets\n",
    "# TEST_SIZE VERÄNDERN\n",
    "TEST_SIZE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c83dbc4-f3e8-44d7-adf8-d28bebb94533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generierung des Datansatzes und split in test und train set\n",
    "X, y = make_moons(n_samples = N_SAMPLES, noise=0.2, random_state=100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195341a9-b061-4fbe-938e-34557ee16eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilfsfunktion um die Daten zu Visualisiern\n",
    "def make_plot(X, y, plot_name, file_name=None, XX=None, YY=None, preds=None, dark=False):\n",
    "    if (dark):\n",
    "        plt.style.use('dark_background')\n",
    "    else:\n",
    "        sns.set_style(\"whitegrid\")\n",
    "    plt.figure(figsize=(16,12))\n",
    "    axes = plt.gca()\n",
    "    axes.set(xlabel=\"$X_1$\", ylabel=\"$X_2$\")\n",
    "    plt.title(plot_name, fontsize=30)\n",
    "    plt.subplots_adjust(left=0.20)\n",
    "    plt.subplots_adjust(right=0.80)\n",
    "    if(XX is not None and YY is not None and preds is not None):\n",
    "        plt.contourf(XX, YY, preds.reshape(XX.shape), 25, alpha = 1, cmap=cm.Spectral)\n",
    "        plt.contour(XX, YY, preds.reshape(XX.shape), levels=[.5], cmap=\"Greys\", vmin=0, vmax=.6)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y.ravel(), s=40, cmap=plt.cm.Spectral, edgecolors='black')\n",
    "    if(file_name):\n",
    "        plt.savefig(file_name)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdb7d55-4aac-4c84-bed4-df6cd2749467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisiung des gesamten Datensatzes\n",
    "make_plot(X, y, \"Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664dd04c-d3d0-4b1e-9548-d9f73998dfd6",
   "metadata": {},
   "source": [
    "## Training des Models und Visualisung der Ergebnisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891ddc1e-983f-451f-adbd-4c0f8be8f339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training des Neuronalen Netzes\n",
    "# 10000 Gradientenschritte\n",
    "# Lernrate 0.01\n",
    "epochs = 10000\n",
    "learning_rate = 0.01\n",
    "params_values, history = train(np.transpose(X_train), np.transpose(y_train.reshape((y_train.shape[0], 1))),np.transpose(X_test), np.transpose(y_test.reshape((y_test.shape[0], 1))), NN_ARCHITECTURE, epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82261dd2-82c4-4b16-9317-0aab1e01303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vorwärtsdurchlauf auf den Testdaten mit den gelernten Gewichten\n",
    "Y_test_hat, _ = full_forward_propagation(np.transpose(X_test), params_values, NN_ARCHITECTURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45faf141-a3cc-4296-9008-30b8937eef32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check der Genauigkeit\n",
    "acc_test = get_accuracy_value(Y_test_hat, np.transpose(y_test.reshape((y_test.shape[0], 1))))\n",
    "print(\"Test set accuracy: {:.2f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5232e3f-a0ea-4205-8070-32f6ab0d0ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learn_data(data1,data2,name=\"Cost\"):\n",
    "    plt.figure(figsize=(16,12))\n",
    "    axes = plt.gca()\n",
    "    axes.set(xlabel=\"$Epochen$\", ylabel=name)\n",
    "    plt.title(name, fontsize=30)\n",
    "    plt.subplots_adjust(left=0.20)\n",
    "    plt.subplots_adjust(right=0.80)\n",
    "    plt.plot(data1)\n",
    "    plt.plot(data2)\n",
    "    axes.legend(['Train Set', 'Test Set'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e90e0fc-5e6c-40be-8b64-ed01e348b371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot der Kostenfunktion\n",
    "plot_learn_data(history[0],history[2])\n",
    "# Plot Genauigkeit\n",
    "plot_learn_data(history[1],history[3],name=\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dfbd7a-24c1-46ba-b14b-fb5ebee11581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hilfsvariablen für die Visualisierung\n",
    "GRID_X_START = -1.5\n",
    "GRID_X_END = 2.5\n",
    "GRID_Y_START = -1.0\n",
    "GRID_Y_END = 2\n",
    "\n",
    "# Erstellen eines Gitteres für das Bild\n",
    "grid = np.mgrid[GRID_X_START:GRID_X_END:100j,GRID_X_START:GRID_Y_END:100j]\n",
    "grid_2d = grid.reshape(2, -1).T\n",
    "XX, YY = grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f74fd05-532c-44aa-83cd-a88972c47453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vorwärtsdurchlauf des Netzes mit dem gelernten Gewichten auf dem Gitter\n",
    "prediction_probs_numpy, _ = full_forward_propagation(np.transpose(grid_2d), params_values, NN_ARCHITECTURE)\n",
    "prediction_probs_numpy = prediction_probs_numpy.reshape(prediction_probs_numpy.shape[1], 1)\n",
    "# Visualisierung der gelernten Gewichte und dem Test set\n",
    "make_plot(X_test, y_test, \"NumPy Model mit gelernten Gewichten\", file_name=None, XX=XX, YY=YY, preds=prediction_probs_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cc1a64-960f-4b94-8fda-90c8bc82348c",
   "metadata": {},
   "source": [
    "## Visualisiung der initialen Gewichte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef854861-8b39-45b4-9acd-2bc18f43dc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_values = init_layers(NN_ARCHITECTURE, 2)\n",
    "prediction_probs_numpy, _ = full_forward_propagation(np.transpose(grid_2d), init_values, NN_ARCHITECTURE)\n",
    "prediction_probs_numpy = prediction_probs_numpy.reshape(prediction_probs_numpy.shape[1], 1)\n",
    "# Visualisierung der initialen Gewichte und dem Test set\n",
    "make_plot(X_test, y_test, \"NumPy Model mit initialen Gewichten\", file_name=None, XX=XX, YY=YY, preds=prediction_probs_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d48386-7d32-4862-89da-1b7d11a18538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
